{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/phuonglh/vlp/dat/nli/XNLI-1.0/vi.tok.jsonl\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "path = home + \"/vlp/dat/nli/XNLI-1.0/vi.tok.jsonl\"\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VietnameseXNLI(Dataset):\n",
    "    def __init__(self, jsonlPath):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        with open(jsonlPath) as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line)\n",
    "                self.X.append(sample[\"sentence1_tokenized\"] +\n",
    "                              \" </s> \" + sample[\"sentence2_tokenized\"])\n",
    "                self.y.append(sample[\"gold_label\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VietnameseXNLI(path)\n",
    "N = int(0.8*len(dataset))\n",
    "training, test = random_split(dataset, [N, len(dataset)-N], generator=Generator().manual_seed(12345))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing only\n",
    "train_loader = DataLoader(training, batch_size=batch_size)\n",
    "test_loader = DataLoader(test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ừm không có gì sai với việc một phụ_huynh tặng tất_cả mọi thứ cô ấy có để ừ cho ừ cho một cá_nhân như ừ như bạn </s> Bố_mẹ có_thể cho con_cái nhiều quà_cáp .',\n",
       "  'Ngày_nay , những người Đức này thậm_chí không ở lại Hoa_Kỳ . </s> Những người Đức này được sử_dụng trên khắp châu Mỹ ngày này .',\n",
       "  'tôi sợ là , tôi nghĩ rằng tên anh ấy_là Anderson , một quý ông đang tìm_kiếm một chiếc vé độc_lập chống lại Reagan </s> Anderson đánh_bại Reagan .',\n",
       "  'Nếu có_khi nào tôi viết một cuốn tự_truyện , nó sẽ là từ_điển của những địa_danh và con_người được định_nghĩa theo tầm quan_trọng riêng . </s> Điều quan_trọng với tôi là tự_truyện của tôi có_thể được tiếp_cận dễ_dàng nhất .',\n",
       "  'Trong xe_ngựa kéo , bạn ghé thăm dân_làng đang thu_hoạch , cắt lông cừu , mài bột trong nhà_máy , dệt , và làmt móng ngựa . </s> Người_dân trong làng ăn_mặc như thời thuộc_địa .',\n",
       "  'Những người không giao_tiếp với các ngôn_ngữ liên_quan có_lẽ sẽ không có câu trả_lời cho những câu_hỏi tu_từ này , nhưng tôi cảm_thấy chắc_chắn rằng họ sẽ thích được tha_thứ sự_thật đáng ghét . </s> Những người không nói được ngôn_ngữ sẽ khó trả_lời .',\n",
       "  'Chẳng có gì ngoài sa_mạc ; có cây đan sâm trên đường đi . </s> Con đường vắng thật vắng_vẻ .',\n",
       "  'um tôi có một đứa con_gái nhỏ mười_tám tháng tuổi </s> Tôi có đông con .',\n",
       "  'Ít người biết đến của những cư_dân thời_kỳ đồ đá sớm nhất ở cực tây nam châu Âu . </s> Những người sống ở châu Âu trong thời_kỳ Đồ đá .',\n",
       "  'Không cần phải xin_lỗi vì vai_trò lãnh_đạo của chúng_tôi . </s> Lựa_chọn duy_nhất của chúng_tôi là tuân theo mọi hướng mà chúng_tôi được chỉ cho từ cấp trên và cấp cao hơn_nữa .',\n",
       "  'Nhưng nó không dừng lại trước dự_định của tay súng . </s> Pháo_thủ đã định bắn vũ_khí của anh_ta .',\n",
       "  'Các faaade của đền thờ Ramses_II là một trong những hình_ảnh lâu_dài nhất của Ai_Cập và mặc_dù bạn có_thể đã nhìn thấy chúng trong các bức ảnh , họ thực_sự ngoạn_mục trong thực_tế . </s> Các faaade là trong lăng_mộ của vua Tut .',\n",
       "  'Trong các tổ_chức hàng_đầu , phát_triển các quy_trình kinh_doanh đóng một vai_trò quan_trọng trong việc xác_định cách các trách_nhiệm quản_lý thông_tin được cấu_trúc và điều_chỉnh để đáp_ứng nhu_cầu thay_đổi . </s> Vì nhu_cầu không_bao_giờ thay_đổi , các quy_trình nghiệp_vụ không cần phải tiến hóa .',\n",
       "  'Kẻ bắt_giữ hẹn gặp Slahi vào tháng 10 năm 1999 . </s> Cuộc họp Slahi được tổ_chức vào tháng 10 năm 1999 .',\n",
       "  'Cả hai loại này đều có_thể được thay_đổi mà không làm thay_đổi cơ_chế đối_xứng codon-codon . </s> Những thay_đổi rất khắc_nghiệt .',\n",
       "  'Quần_đảo Saronic có mùa kéo dài_lâu hơn trải dài từ tháng Tư đến tháng Mười . </s> Các đảo Saronic có một mùa rõ_rệt .',\n",
       "  'phải , tôi cũng không hiểu điều gì khiến , anh biết đấy , sự_việc đã lao dốc để bị đánh_bại vào tháng 12 năm_ngoái bởi có bao_nhiêu phiếu bầu chỉ bằng vài trăm phiếu </s> Nó đạt tỉ_lệ 99 % .',\n",
       "  'Làm cho đầu của bạn với nhiệt Do_đó , tôi biết rằng bạn đi đến độ dài thêm để được từ_bi và chăm_sóc cho người khác . </s> Tôi biết bạn bỏ nhiều tiền để nuôi những người đang đói xung_quanh bạn .',\n",
       "  'Thêm vào đó , như chúng_ta đã biết , cuộc_sống xuất_hiện trên Trái_Đất chỉ một lần . </s> Cuộc_sống trên trái_đất có_thể đã xuất_hiện nhiều lần .',\n",
       "  'Thật vậy , Bios_Group có liên_quan đến việc phát_minh và tạo ra chúng . </s> Bios_Group đã dành rất nhiều tiền thô vào sáng_tạo của họ .',\n",
       "  'và sau đó điều thứ_hai tôi có_thể xem là những gì họ có_thể mua được </s> Tôi cũng nhìn vào những thứ có giá_cả phải_chăng đối_với họ .',\n",
       "  \"Điều rất thú_vị rằng Hillary_Rodham_Clinton có_thể có bất_cứ điều gì để học_hỏi từ Công_nương Diana là đủ hấp_dẫn để làm cho tôi nhấn vào Margaret_Carlson ' s Hillary và Di . </s> Hillary_Rodham_Clinton có_thể học_hỏi từ Công_nương Diana .\",\n",
       "  'Triết_lý pháp_lý của Liên_minh chiến_thắng , cả về chất và phong_cách . </s> Triết_lý pháp_lý đã chiến_thắng cả về mặt chất và phong_cách .',\n",
       "  'Một nhóm tại Hiệp_hội Quán_Bar của Thành_phố New_York , trong khi đó , đã thảo_luận về khoản nợ của sinh_viên trong sáu tháng . </s> Một người_ở Idaho chưa bao_giờ nghĩ về nợ sinh_viên .',\n",
       "  'Trang_phục bởi Brigitte_Doth , Jade_Stice , và Penny_Laimana . </s> Để tạo ra trang_phục này cần sự hợp sức của cả ba người .',\n",
       "  'Vào ban_đêm có nhiều nhà_hàng , câu_lạc_bộ và nhà_hát hay để thưởng_thức , và vào ban_ngày có bãi_biển rực_rỡ , hoàn_chỉnh với bến_tàu giải_trí , đu_quay cổ và khu mua_sắm gần đó . </s> Có rất nhiều nơi để khám_phá cả ngày lẫn đêm .',\n",
       "  'Tôi đã nói với anh_ấy , tôi đã cố_gắng giải_thích cho anh_ấy rằng tôi đã thất_vọng vì tôi không có đủ thông_tin tôi cần . </s> Tôi nói với anh_ấy tôi không muốn nghe bất_cứ điều gì khác .',\n",
       "  'Thưa ông , đất_nước là tất_cả , chủ_quyền chỉ là vô_ích . </s> Nhiều năm hỗn_loạn đã khiến đất_đai không ổn_định .',\n",
       "  'Ngoài việc nghiên_cứu cẩn_thận các hồ_sơ chính_thức khác_nhau , chính_phủ Séc cũng xem_xét các bức ảnh giám_sát được chụp bên ngoài đại_sứ_quán Iraq . </s> Chính_phủ Séc không có tài_liệu giám_sát .',\n",
       "  'Trong thực_tế , đó là điểm chớp_nhoáng cho các cuộc biểu_tình và bạo_loạn trong cuộc tranh_luận về xe_buýt trong những năm 1970 . </s> Có những cuộc biểu_tình chủng_tộc vào những năm 70 .',\n",
       "  'Các nhà ngôn_ngữ_học viết sách dường_như vẫn luôn_luôn là những học_giả đang muốn quảng_bá quan_điểm riêng của họ , một_số trong số các quan_điểm đó rất khó hiểu , ít_nhất mà nói là vậy . </s> Các nhà ngôn_ngữ_học kiếm rất nhiều tiền từ việc viết sách .',\n",
       "  'Từ một nhóm diễn_viên lưu_diễn năm 1973 bao_gồm sinh_viên ở các thành_phố như Gary , Elkhart , và Terre_Haute , ngày_nay đã trở_thành chương_trình giáo_dục IRT . </s> Một_số diễn_viên lưu_diễn ở Indiana năm 1973 .'),\n",
       " ('entailment',\n",
       "  'neutral',\n",
       "  'contradiction',\n",
       "  'contradiction',\n",
       "  'neutral',\n",
       "  'entailment',\n",
       "  'neutral',\n",
       "  'contradiction',\n",
       "  'entailment',\n",
       "  'contradiction',\n",
       "  'neutral',\n",
       "  'contradiction',\n",
       "  'contradiction',\n",
       "  'entailment',\n",
       "  'neutral',\n",
       "  'entailment',\n",
       "  'contradiction',\n",
       "  'neutral',\n",
       "  'neutral',\n",
       "  'neutral',\n",
       "  'entailment',\n",
       "  'entailment',\n",
       "  'entailment',\n",
       "  'contradiction',\n",
       "  'entailment',\n",
       "  'entailment',\n",
       "  'contradiction',\n",
       "  'neutral',\n",
       "  'contradiction',\n",
       "  'neutral',\n",
       "  'neutral',\n",
       "  'entailment')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='vinai/phobert-base', vocab_size=64000, model_max_len=256, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    BertPreTrainedModel,\n",
    "    RobertaConfig\n",
    ")\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaClassificationHead,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    ")\n",
    "import torchtext\n",
    "from torch.nn import CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaPhoBERTClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(RobertaPhoBERTClassifier, self).__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.model = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "        return outputs  # (loss,) logits, (hidden_states, (attentions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaPhoBERTClassifier: ['roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.pooler.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'lm_head.layer_norm.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'lm_head.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaPhoBERTClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaPhoBERTClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaPhoBERTClassifier were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['model.encoder.layer.3.attention.output.dense.bias', 'model.encoder.layer.10.attention.self.value.bias', 'model.encoder.layer.0.attention.self.key.bias', 'model.encoder.layer.2.output.LayerNorm.weight', 'model.encoder.layer.10.intermediate.dense.bias', 'model.encoder.layer.8.attention.self.key.weight', 'model.encoder.layer.4.attention.output.dense.bias', 'model.encoder.layer.4.attention.self.key.weight', 'model.encoder.layer.0.attention.self.query.weight', 'model.encoder.layer.4.attention.self.key.bias', 'model.encoder.layer.4.output.LayerNorm.weight', 'model.encoder.layer.7.attention.self.value.bias', 'model.encoder.layer.2.attention.output.dense.bias', 'model.encoder.layer.7.output.dense.bias', 'model.encoder.layer.11.output.dense.weight', 'classifier.dense.bias', 'model.encoder.layer.11.attention.self.key.bias', 'model.encoder.layer.11.attention.self.value.weight', 'model.encoder.layer.11.output.dense.bias', 'model.encoder.layer.0.intermediate.dense.bias', 'model.encoder.layer.8.attention.output.dense.weight', 'model.encoder.layer.1.attention.output.dense.weight', 'model.encoder.layer.11.output.LayerNorm.bias', 'model.encoder.layer.1.attention.self.key.weight', 'model.encoder.layer.6.intermediate.dense.bias', 'model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.encoder.layer.8.output.dense.weight', 'model.encoder.layer.4.intermediate.dense.bias', 'model.encoder.layer.10.attention.self.query.bias', 'model.encoder.layer.11.intermediate.dense.bias', 'model.encoder.layer.2.attention.output.dense.weight', 'model.encoder.layer.5.intermediate.dense.weight', 'model.encoder.layer.11.output.LayerNorm.weight', 'model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.encoder.layer.8.attention.self.query.bias', 'model.encoder.layer.9.output.dense.weight', 'model.encoder.layer.11.attention.self.query.weight', 'model.encoder.layer.2.attention.self.query.weight', 'model.encoder.layer.4.attention.output.dense.weight', 'model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.encoder.layer.10.output.dense.weight', 'model.encoder.layer.9.intermediate.dense.bias', 'model.encoder.layer.4.attention.self.query.bias', 'model.encoder.layer.2.attention.self.query.bias', 'model.encoder.layer.1.attention.self.query.bias', 'model.encoder.layer.0.attention.self.query.bias', 'model.encoder.layer.6.attention.self.key.bias', 'model.encoder.layer.11.attention.self.query.bias', 'model.encoder.layer.2.output.LayerNorm.bias', 'model.encoder.layer.0.output.dense.weight', 'model.encoder.layer.6.output.dense.weight', 'model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.encoder.layer.4.output.LayerNorm.bias', 'model.encoder.layer.1.attention.self.key.bias', 'model.encoder.layer.0.output.LayerNorm.bias', 'model.encoder.layer.5.attention.self.query.bias', 'model.encoder.layer.4.attention.self.value.bias', 'model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.encoder.layer.1.output.dense.weight', 'model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.encoder.layer.11.attention.self.value.bias', 'model.encoder.layer.9.attention.self.value.bias', 'model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.embeddings.LayerNorm.bias', 'model.encoder.layer.3.attention.self.query.bias', 'model.encoder.layer.3.output.LayerNorm.weight', 'model.encoder.layer.7.output.dense.weight', 'model.encoder.layer.7.attention.output.dense.bias', 'model.encoder.layer.3.attention.output.dense.weight', 'model.encoder.layer.2.attention.self.key.bias', 'model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.encoder.layer.5.output.dense.bias', 'model.encoder.layer.2.attention.self.key.weight', 'model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.encoder.layer.8.attention.self.query.weight', 'model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.encoder.layer.4.attention.self.query.weight', 'model.encoder.layer.11.attention.self.key.weight', 'model.encoder.layer.8.attention.output.dense.bias', 'model.encoder.layer.9.attention.self.query.bias', 'model.encoder.layer.1.output.LayerNorm.bias', 'model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.encoder.layer.6.output.LayerNorm.bias', 'model.encoder.layer.9.attention.self.key.weight', 'model.embeddings.token_type_embeddings.weight', 'model.encoder.layer.1.attention.self.value.bias', 'model.encoder.layer.3.attention.self.value.bias', 'model.encoder.layer.5.attention.self.value.bias', 'model.encoder.layer.3.output.dense.weight', 'model.encoder.layer.6.attention.self.value.weight', 'model.encoder.layer.7.attention.self.key.bias', 'model.encoder.layer.2.attention.self.value.weight', 'model.encoder.layer.3.attention.self.query.weight', 'model.encoder.layer.3.attention.self.key.weight', 'model.encoder.layer.7.attention.self.query.weight', 'model.encoder.layer.7.output.LayerNorm.bias', 'model.encoder.layer.10.output.dense.bias', 'model.encoder.layer.2.output.dense.bias', 'model.encoder.layer.9.attention.output.dense.weight', 'model.encoder.layer.6.output.dense.bias', 'model.encoder.layer.10.attention.output.dense.bias', 'model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.encoder.layer.4.intermediate.dense.weight', 'model.encoder.layer.6.intermediate.dense.weight', 'model.encoder.layer.4.output.dense.bias', 'model.encoder.layer.9.output.LayerNorm.bias', 'model.encoder.layer.7.attention.self.value.weight', 'model.encoder.layer.4.output.dense.weight', 'model.encoder.layer.5.attention.output.dense.bias', 'model.encoder.layer.0.output.LayerNorm.weight', 'model.encoder.layer.10.attention.self.value.weight', 'model.encoder.layer.2.output.dense.weight', 'model.encoder.layer.2.intermediate.dense.weight', 'model.encoder.layer.6.attention.self.query.bias', 'model.encoder.layer.5.intermediate.dense.bias', 'model.encoder.layer.5.attention.self.query.weight', 'model.encoder.layer.8.intermediate.dense.weight', 'model.encoder.layer.6.attention.output.dense.weight', 'model.encoder.layer.10.intermediate.dense.weight', 'model.encoder.layer.2.intermediate.dense.bias', 'model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.encoder.layer.4.attention.self.value.weight', 'model.encoder.layer.1.attention.output.dense.bias', 'model.encoder.layer.1.output.LayerNorm.weight', 'model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.encoder.layer.9.attention.self.value.weight', 'model.encoder.layer.0.attention.output.dense.weight', 'model.encoder.layer.1.output.dense.bias', 'model.encoder.layer.9.output.dense.bias', 'model.encoder.layer.8.output.LayerNorm.bias', 'model.encoder.layer.8.attention.self.key.bias', 'model.encoder.layer.9.attention.output.dense.bias', 'model.encoder.layer.11.attention.output.dense.bias', 'model.encoder.layer.5.attention.self.value.weight', 'model.encoder.layer.3.intermediate.dense.bias', 'model.encoder.layer.8.intermediate.dense.bias', 'model.encoder.layer.10.attention.self.key.bias', 'model.encoder.layer.1.attention.self.value.weight', 'model.encoder.layer.0.intermediate.dense.weight', 'model.encoder.layer.10.output.LayerNorm.bias', 'model.encoder.layer.7.attention.self.key.weight', 'model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.encoder.layer.6.attention.output.dense.bias', 'model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.encoder.layer.1.intermediate.dense.weight', 'model.embeddings.position_embeddings.weight', 'model.encoder.layer.1.attention.self.query.weight', 'model.encoder.layer.9.output.LayerNorm.weight', 'model.embeddings.LayerNorm.weight', 'classifier.out_proj.bias', 'model.encoder.layer.10.attention.self.key.weight', 'model.encoder.layer.3.output.LayerNorm.bias', 'classifier.out_proj.weight', 'model.encoder.layer.5.output.LayerNorm.weight', 'model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.embeddings.word_embeddings.weight', 'model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.encoder.layer.11.attention.output.dense.weight', 'model.encoder.layer.10.attention.self.query.weight', 'model.encoder.layer.5.attention.output.dense.weight', 'model.encoder.layer.11.intermediate.dense.weight', 'model.encoder.layer.3.attention.self.value.weight', 'model.encoder.layer.8.output.LayerNorm.weight', 'model.encoder.layer.10.output.LayerNorm.weight', 'model.encoder.layer.7.intermediate.dense.weight', 'model.encoder.layer.8.attention.self.value.weight', 'classifier.dense.weight', 'model.encoder.layer.7.output.LayerNorm.weight', 'model.encoder.layer.3.output.dense.bias', 'model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.encoder.layer.0.output.dense.bias', 'model.encoder.layer.3.intermediate.dense.weight', 'model.encoder.layer.1.intermediate.dense.bias', 'model.encoder.layer.5.output.LayerNorm.bias', 'model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.encoder.layer.8.output.dense.bias', 'model.encoder.layer.9.attention.self.key.bias', 'model.encoder.layer.0.attention.self.value.bias', 'model.encoder.layer.6.attention.self.query.weight', 'model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.pooler.dense.bias', 'model.encoder.layer.2.attention.self.value.bias', 'model.encoder.layer.0.attention.self.value.weight', 'model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.encoder.layer.5.attention.self.key.bias', 'model.encoder.layer.7.intermediate.dense.bias', 'model.encoder.layer.9.intermediate.dense.weight', 'model.encoder.layer.9.attention.self.query.weight', 'model.encoder.layer.6.attention.self.value.bias', 'model.encoder.layer.5.output.dense.weight', 'model.encoder.layer.8.attention.self.value.bias', 'model.encoder.layer.6.output.LayerNorm.weight', 'model.encoder.layer.0.attention.output.dense.bias', 'model.encoder.layer.7.attention.self.query.bias', 'model.encoder.layer.3.attention.self.key.bias', 'model.encoder.layer.0.attention.self.key.weight', 'model.pooler.dense.weight', 'model.encoder.layer.7.attention.output.dense.weight', 'model.encoder.layer.6.attention.self.key.weight', 'model.encoder.layer.10.attention.output.dense.weight', 'model.encoder.layer.5.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaPhoBERTClassifier(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pchanda.github.io/Roberta-FineTuning-for-Classification/\n",
    "config_class = RobertaConfig\n",
    "model_class = RobertaPhoBERTClassifier\n",
    "config = config_class.from_pretrained(\"vinai/phobert-base\", num_labels=3)\n",
    "model = model_class.from_pretrained(\"vinai/phobert-base\", config=config)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        xs, ys = data\n",
    "        # this is like calling tokenizer.encode() but has paddings\n",
    "        self.examples = tokenizer(text=xs, text_pair=None, truncation=True, padding='max_length',\n",
    "                                  max_length=tokenizer.model_max_length, return_tensors='pt')\n",
    "        self.labels = torch.tensor([label_dict[y] for y in ys], dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: self.examples[key][index] for key in self.examples}, self.labels[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each data is a pair of (xs, ys)\n",
    "training_data = ([x[0] for x in training], [x[1] for x in training])\n",
    "test_data = ([x[0] for x in test], [x[1] for x in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = ClassificationDataset(training_data, tokenizer)\n",
    "test_ds = ClassificationDataset(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "train_loader = DataLoader(training_ds, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test_batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 14583,   599,  ...,     1,     1,     1],\n",
       "         [    0,  6177,     4,  ...,     1,     1,     1],\n",
       "         [    0,    70,  1080,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,    92,   573,  ...,     1,     1,     1],\n",
       "         [    0,   146,    69,  ...,     1,     1,     1],\n",
       "         [    0,   404,    16,  ...,     1,     1,     1]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xs in the first batch\n",
    "first_test_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 2, 1, 0, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2,\n",
       "        0, 0, 2, 1, 2, 1, 1, 0])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ys in the first batch\n",
    "first_test_batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_dict(batch):\n",
    "    inputs = {key: value.squeeze(1).to(DEVICE) for key, value in batch[0].items()}\n",
    "    inputs[\"labels\"] = batch[1].to(DEVICE)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 14583,   599,  ...,     1,     1,     1],\n",
       "         [    0,  6177,     4,  ...,     1,     1,     1],\n",
       "         [    0,    70,  1080,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,    92,   573,  ...,     1,     1,     1],\n",
       "         [    0,   146,    69,  ...,     1,     1,     1],\n",
       "         [    0,   404,    16,  ...,     1,     1,     1]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([0, 1, 2, 2, 1, 0, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2,\n",
       "         0, 0, 2, 1, 2, 1, 1, 0])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_inputs_dict(first_test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_inputs_dict(first_test_batch)\n",
    "first_input_ids = batch['input_ids'].to(DEVICE)\n",
    "first_attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "first_labels = batch['labels'].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 14583,   599,  ...,     1,     1,     1],\n",
      "        [    0,  6177,     4,  ...,     1,     1,     1],\n",
      "        [    0,    70,  1080,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,    92,   573,  ...,     1,     1,     1],\n",
      "        [    0,   146,    69,  ...,     1,     1,     1],\n",
      "        [    0,   404,    16,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0, 1, 2, 2, 1, 0, 1, 2, 0, 2, 1, 2, 2, 0, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2,\n",
      "        0, 0, 2, 1, 2, 1, 1, 0])}\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0939, grad_fn=<NllLossBackward0>),\n",
       " tensor([[0.2798, 0.1798, 0.2442],\n",
       "         [0.2941, 0.1732, 0.3278],\n",
       "         [0.2628, 0.2264, 0.3528],\n",
       "         [0.2902, 0.1886, 0.3668],\n",
       "         [0.2658, 0.2300, 0.3313],\n",
       "         [0.2819, 0.1992, 0.3798],\n",
       "         [0.3302, 0.1284, 0.3000],\n",
       "         [0.3033, 0.1471, 0.3298],\n",
       "         [0.2693, 0.1761, 0.3628],\n",
       "         [0.2615, 0.1826, 0.3168],\n",
       "         [0.2951, 0.1279, 0.2781],\n",
       "         [0.2566, 0.2005, 0.3472],\n",
       "         [0.2596, 0.1991, 0.3500],\n",
       "         [0.2887, 0.1695, 0.2419],\n",
       "         [0.2949, 0.1784, 0.3382],\n",
       "         [0.3207, 0.1255, 0.3589],\n",
       "         [0.2571, 0.1994, 0.3595],\n",
       "         [0.2897, 0.1694, 0.3244],\n",
       "         [0.2506, 0.2077, 0.2737],\n",
       "         [0.2984, 0.1981, 0.3337],\n",
       "         [0.2868, 0.1829, 0.3259],\n",
       "         [0.2960, 0.1705, 0.3220],\n",
       "         [0.3104, 0.0864, 0.3696],\n",
       "         [0.2798, 0.1910, 0.2853],\n",
       "         [0.2929, 0.2194, 0.4174],\n",
       "         [0.3161, 0.1978, 0.3310],\n",
       "         [0.3340, 0.1649, 0.3496],\n",
       "         [0.2489, 0.2055, 0.3044],\n",
       "         [0.2470, 0.2250, 0.3659],\n",
       "         [0.2787, 0.2177, 0.2787],\n",
       "         [0.2842, 0.2320, 0.3484],\n",
       "         [0.3205, 0.1713, 0.3646]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for testing\n",
    "model(first_input_ids, first_attention_mask, first_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_ratio = 0.06\n",
    "weight_decay=0.0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 15\n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08\n",
    "t_total = len(train_loader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(preds, model_outputs, labels, eval_examples=None, multi_label=False):\n",
    "    assert len(preds) == len(labels)\n",
    "    mismatched = labels != preds\n",
    "    wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
    "    scores = np.array([softmax(element)[1] for element in model_outputs])\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    auprc = average_precision_score(labels, scores)\n",
    "    return (\n",
    "        {\n",
    "            **{\"mcc\": mcc, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"auroc\": auroc, \"auprc\": auprc},\n",
    "        },\n",
    "        wrong,\n",
    "    )\n",
    "\n",
    "def print_confusion_matrix(result):\n",
    "    print('confusion matrix:')\n",
    "    print('            predicted    ')\n",
    "    print('          0     |     1')\n",
    "    print('    ----------------------')\n",
    "    print('   0 | ',format(result['tn'],'5d'),' | ',format(result['fp'],'5d'))\n",
    "    print('gt -----------------------')\n",
    "    print('   1 | ',format(result['fn'],'5d'),' | ',format(result['tp'],'5d'))\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = get_inputs_dict(batch)\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    # evaluate model on test data at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(test_loader)\n",
    "    preds = np.empty((len(test_ds), 3))\n",
    "    out_label_ids = np.empty((len(test_ds)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i, test_batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            test_batch = get_inputs_dict(test_batch)\n",
    "            input_ids = test_batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = test_batch['attention_mask'].to(DEVICE)\n",
    "            labels = test_batch['labels'].to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = batch_size * i\n",
    "        end_index = start_index + batch_size if i != (n_batches - 1) else len(test_ds)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    result, wrong = compute_metrics(preds, model_outputs, out_label_ids, test_data)\n",
    "    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result)\n",
    "    print_confusion_matrix(result)\n",
    "    print('---------------------------------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
