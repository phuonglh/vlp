1. Token Model
    
  The input to its BigDL model is a batch of sequences, each sequence is of length maxSeqLen containing integers 
  corresponding to the indices of the tokens in the sequence, obtained via a token dictionary.

    token => Embedding(d) -> RNN(d => r, returnSeq=true) -> TimeDistributed(Dense(r => h)) ->
        Dropout(p) -> TimeDistributed(Dense(h => numLabels, softmax))

  INP: [Tom, loves, Jerry] ==> [i(Tom), i(loves), i(Jerry)]


2. Character Model

  As in the semi-character model, each token is decomposed into 3 parts [b, i, e]. Then each part is represented by a binary-valued 
  vector. v(b) and v(e) are one-hot vectors of length d. v(i) is a multi-hot vector, also of length d. Here, d is the size of the alphabet.

  The input to its BigDL model is a batch of sequence, each sequence is of length (3*maxSeqLen*d). 

    Reshape(3*maxSeqLen*d => (maxSeqLen, 3*d)) -> RNN(3*d => r, returnSeq=true) -> TimeDistributed(Dense(r => h)) ->
      Dropout(p) -> TimeDistributed(Dense(h => numLabels, softmax))
  
  INP: [Tom, loves, Jerry] => [b(Tom), m(Tom), e(Tom),..., b(Jerry), m(Jerry), e(Jerry)]

  b(Jerry) = one-hot-vector(J)
  e(Jerry) = one-hot-vector(y)
  m(Jerry) = one-hot-vector(e) + one-hot-vector(r) + one-hot-vector(r)
 
===