1. Token Model
    
  The input to its BigDL model is a batch of sequences, each sequence is of length maxSeqLen containing integers 
  corresponding to the indices of the tokens in the sequence, obtained via a token dictionary.

    token => Embedding(d) -> RNN(d => r, returnSeq=true) -> TimeDistributed(Dense(r => h)) ->
        Dropout(p) -> TimeDistributed(Dense(h => numLabels, softmax))

  INP: [Tom, loves, Jerry] ==> [i(Tom), i(loves), i(Jerry)]


2. Character Model

  As in the semi-character model, each token is decomposed into 3 parts [b, i, e]. Then each part is represented by a binary-valued 
  vector. v(b) and v(e) are one-hot vectors of length d. v(i) is a multi-hot vector, also of length d. Here, d is the size of the alphabet.

  The input to its BigDL model is a batch of sequence, each sequence is of length (3*maxSeqLen*d). 

    Reshape(3*maxSeqLen*d => (maxSeqLen, 3*d)) -> RNN(3*d => r, returnSeq=true) -> TimeDistributed(Dense(r => h)) ->
      Dropout(p) -> TimeDistributed(Dense(h => numLabels, softmax))
  
  INP: [Tom, loves, Jerry] => [b(Tom), m(Tom), e(Tom),..., b(Jerry), m(Jerry), e(Jerry)]

  b(Jerry) = one-hot-vector(J)
  e(Jerry) = one-hot-vector(y)
  m(Jerry) = one-hot-vector(e) + one-hot-vector(r) + one-hot-vector(r)
 
===

1. Token Representation for BERT

  A table of 4 tensors:
    - tokenIds (using a vocabulary, same as in the Token Model)
    - segmentIds (all have value 0 since we only have one sentence)
    - positionIds (which range in [0 to maxSeqLen) interval)
    - masks (all have value 1)

  The tokenIds tensor is obtained via the same preprocessing as in the Token Model. The Sequence4BERT 
  preprocessor create a feature vector which is the concatenation of the 4 tensors above:

    tokenIds::segmentIds::positionIds::masks

  An appropriate feature size array is used to help convert this feature vector into a table of 4 inputs for BERT to work, each 
  input has an input shape Array(maxSeqLen). Note that we use a Reshape layer to reshape the masks input to 
  targetShape = Array(1, 1, maxSeqLen). 

2. Character Representation for BERT

  Jerry => [J, err, y] => 3 subtokens


===

M1. Token Model with LSTM: [Sequencer, LSTM] (input is a vector of token ids)
M2. Token Model with BERT: [Sequencer4BERT, BERT] (input is a vector of token ids)
M3. Character Model with LSTM: [MultiHotSequencer, LSTM] (input is a multi-hot vector)
M4. Subsyllable Model with BERT: [SubsyllableSequencer, BERT] (input is a vector of subsyllable ids)


Conference paper: M1 and M2
Journal paper: M2, M3 and M4.